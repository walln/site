---
name: Universal Pretraining - A Study on the Influence of Modality on Transformer Pretraining
dateUpdated: 2023-01-25
description: Research I worked on during my graduate studies at SMU investigating the influence of modality on transformer pretraining. This work was never published as it has some issues. Additionally, dataset distillation and foundation models will likely overtake this research direction.
---

<TextBlock
  text="Late into my undergraduate studies at SMU and into my early work on my Master's degree, I worked on some research into transformers and the value that certain data types have. During this research, we evaluated pretraining with different data modalities across different transformer architectures to explore the importance of data. 
This was inspired by An Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale and Pretrained Transformers As Universal Computation Engines. We wrote a paper that we decided not to release or submit for publication as a lot of work could be done to improve our evaluation methodology, paper quality, and lots of ablation studies to prove our findings. So we went separate ways for our future careers and have yet to further develop the research. If you are interested in reading the paper in its current state, you can find it here:"
/>

<Link label="Paper" href="/static/universal-pretraining-research.pdf" />

<TextBlock text="If you are interested the code can be found here:" />

<Link
  label="Github"
  href="https://github.com/walln/cross-modality-pretraining"
/>

<TextBlock text="If you are interested in other research, I have done or am working on, feel free to reach out." />
