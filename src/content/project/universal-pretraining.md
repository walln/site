---
title: Universal Pretraining - Exploring the value of modality
publishDate: 2023-01-25
description: Research I worked on during my graduate studies at SMU investigating the influence of modality on transformer pretraining. This work was abandoned.
---

Late into my undergraduate studies at SMU and into my early work on my Master's degree, I worked on some research into transformers and the value that certain data types have. During this research, we evaluated pretraining with different data modalities across different transformer architectures to explore the importance of data. 
This was inspired by An Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale and Pretrained Transformers As Universal Computation Engines. We wrote a paper that we decided not to release or submit for publication as a lot of work could be done to improve our evaluation methodology, paper quality, and lots of ablation studies to prove our findings. So we went separate ways for our future careers and have yet to further develop the research. If you are interested in reading the paper in its current state, you can find it [here](/universal-pretraining-research.pdf)

If you are interested the code can be found on [GitHub](https://github.com/walln/cross-modality-pretraining)

If you are interested in other research, I have done or am working on, feel free to reach out.