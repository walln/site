---
title: Universal Pretraining - Exploring the Value of Modality
publishDate: 2023-01-25
description: Research I worked on during my graduate studies at SMU investigating the influence of modality on transformer pretraining. This work was abandoned.
tags: ["llms", "machine-learning", "research"]
updatedDate: 2024-04-30
---

##  About the Project

Late into my undergraduate studies at SMU and into my early work on my Master's degree, I worked on some research into transformers and the value that certain data types have. During this research, we evaluated pretraining with different data modalities across different transformer architectures to explore the importance of data. 
This was inspired by [An Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) and [Pretrained Transformers As Universal Computation Engines](https://arxiv.org/abs/2103.05247).  The idea was to explore if different modalities would inherently cause the model to improve at a faster rate. This was early exploration of the work and would eventually be taken over by a new direction. That new direction being exploring the importance of data mixtures in multimodal pretraining (what are the impacts of the mixture?). This research was abandonded for several reasons, the methodology was deeply flawed and to get this methodology to get meaningful results would require too much compute and ablation effor. Additionally, the authors went our separate ways for our future careers and have yet to further develop the research. We did have a paper in progress that we did not release or submit for publication (it was still a nice project to submit for some grad school work though) and if you are interested in reading it in its current state you can find it [here](/universal-pretraining-research.pdf)

If you are interested the code can be found on [GitHub](https://github.com/walln/cross-modality-pretraining)

If you are interested in other research, I have done or am working on, feel free to reach out. I am also open to any and all research positions.
